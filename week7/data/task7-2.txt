1. Качество логистической регрессии с коэффициентом C=0,01 над всеми исходными признаками составляет 71,6%.

2. После удаления категориальных признаков улучшения результата не наблюдалось, лишь некоторое ускорение обучения, что вполне объясняется тем, что категориальные признаки как таковые не несут формализуемую числовую нагрузку, но при их удалении анализируемая выборка уменьшается.

            Score, %    Time, mm:ss
   Pure     71,6        00:58
   Clean    71,6        00:52
   Heroes   75,2        04:24

   В целом, модель, построенная логистической регрессией, показывает результаты лучше, чем градиентный бустинг (71,6% против 63,2%), при этом в самом простом случае времени требуется меньше.

3. В выборке найдено 108 различных идентификаторов героев.

4. После добавления "мешка слов" к признакам удалось получить качество 75,1% (коэффициент C=0,1), что лучше на 3,5%. При этом продолжительность обучения увеличилась более чем в 4 раза. Очевидно, что попытка учесть качество каждого конкретного героя путем увеличения или уменьшения веса той или иной команды дает положительный результат.

5. После обучения алгоритма предсказание на тестовой выборке показало, что минимальная вероятность выигрыша команды Radiant составляет 0,9% для игры 33469, максимальная - 99,7% для игры 4320. В среднем команда Radiant должна выиграть 51,8% матчей.

Для логистической регрессии выбран параметр solver="sag", т.к. по условию задания требуется использовать L2-регуляризацию. Метод Stochastic Average Gradient выполняет эту задачу. Одним из требований метода является нормализация данных, поэтому все числовые данные масштабируются до значений в диапазоне -1.0 .. 1.0.
