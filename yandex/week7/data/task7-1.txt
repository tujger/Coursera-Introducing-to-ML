1. Признаки, имеющие пропуски:

    first_blood_time               77677
    first_blood_team               77677
    first_blood_player1            77677
    first_blood_player2            53243
    radiant_bottle_time            81539
    radiant_courier_time           96538
    radiant_flying_courier_time    69751
    radiant_first_ward_time        95394
    dire_bottle_time               81087
    dire_courier_time              96554
    dire_flying_courier_time       71132
    dire_first_ward_time           95404

   Берем первые признаки.
   first_blood_time - игровое время первой крови. Согласно описанию скрипта extract_features.py, извлекаются данные за первые 5 минут игры. Если событие не произошло, то значение пропускается. Очевидно, что в течение первых пяти минут игры совершенно не обязательно должны произойти схватки между игроками, соответственно, и первой крови не будет. По той же самой причине следующие три признака в списке пропускаются.
   Однако признак first_blood_player2 имеет значение еще меньше, чем first_blood_player1. Это объясняется тем, что некоторые игроки могли неоднократно вступать в первую схватку с игроками противоположной фракции (для противника это была первая схватка, для данного игрока - вторая, третья и т.д.).

2. Целевая переменная - radiant_win

3. Кросс-валидация по 5 блокам для градиентного бустинга на 30 деревьях заняла 1 минуту 22 секунды. При этом по метрике AUC-ROC получено качество 63,2%.

4. В зависимости от количества деревьев, получается следующее качество оценки:

    N       Score, %    Time, mm:ss
    5       58,2        00:15
    10      60,7        00:25
    15      61,9        00:38
    20      62,5        00:47
    25      62,9        01:03
    30      63,2        01:08
    35      63,4        01:23
    40      63,6        01:33
    45      63,8        01:43
    50      63,8        01:52
    55      64,0        02:08
    60      64,0        02:29

Таким образом, можно сделать вывод, что при увеличении количества деревьев больше 30 адекватного прироста качества оценки в данной задаче не наблюдается, при этом затраты времени на обучение возрастают пропорционально.

Для оптимизации задачи можно уменьшить количество признаков, а также объединить некоторые однотипные признаки.

Если провести обучение DecisionTreeClassifier на тренировочных данных, то обнаруживается, что наибольшие веса имеют признаки:
    r1_gold, r2_gold, r3_gold, r4_gold, r5_gold,
    d1_gold, d2_gold, d3_gold, d4_gold, d5_gold.

Объединим их в признаки r_mean_gold и d_mean_gold, а исходные столбцы исключим из выборки. Новый анализ показывает, что новые синтетические признаки получили наибольшие веса, а кроме того, выявились следующие однотипные признаки:
    r1_xp, r2_xp, r3_xp, r4_xp, r5_xp,
    d1_xp, d2_xp, d3_xp, d4_xp, d5_xp.

Также объединим их в признаки r_mean_xp и d_mean_xp, а исходные признаки исключим из выборки. Теперь оценка вклада признаков в результат показывает, что первые 10 признаков вносят 40% в конечный результат.

    Признаков   Суммарный вклад, %
    10          40,2
    20          58,9
    30          74,2
    40          84,5
    50          90,1

Проверим два варианта главных компонент - 10 и 50 признаков.

Если оставить 10 наиболее значимых признаков, получим следующую таблицу качества (trace-1-10.txt):

    N       Score, %    Time, mm:ss
    5       64,1        00:02
    10      64,6        00:04
    15      64,8        00:06
    20      64,9        00:08
    25      65,1        00:10
    30      65,0        00:12
    35      65,1        00:13
    40      65,0        00:15
    45      65,1        00:17
    50      65,1        00:18
    55      65,1        00:20
    60      65,1        00:22

При 50 признаках (trace-1-50.txt):

    N       Score, %    Time, mm:ss
    5       64,1        00:08
    10      64,7        00:14
    15      64,8        00:21
    20      64,8        00:26
    25      64,9        00:33
    30      65,0        00:39
    35      65,1        00:44
    40      65,1        00:50
    45      65,2        00:55
    50      65,2        01:00
    55      65,2        01:08
    60      65,2        01:12

Очевидно, что практически никакой разницы в качестве между рассмотренными двумя вариантами нет, при этом затраты времени отличаются в 3-4 раза. Кроме того, достигнутое качество несколько выше, чем в первом неотфильтрованном варианте. Устойчивое качество достигается на 25-30 деревьях, дальнейшее увеличение их количества не приводит к улучшению результата.

Таким образом, наилучшая модель при градиентном бустинге получается после введения синтетических признаков, применении метода главных компонент на 10 признаках и обучения на 25-30 деревьях.